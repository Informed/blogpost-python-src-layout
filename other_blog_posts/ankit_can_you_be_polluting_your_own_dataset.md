# Can you be polluting your own Dataset?

_Ankit Goyal_

<div style="text-align:center">
   <img src="images/main_image.jpg" alt= “curves” width="800" height="500" title="">
</div>

[Photo By: Matthew Henry](https://burst.shopify.com/photos/pouring-cream-into-coffee?q=mixing)

We all have been in those situations during Model Training, when the Validation Accuracy will not increase beyond a certain point or will increase frustratingly slowly. In some cases the Validation Curve will fluctuate up and down. I am not talking about the small noise but the crazy fluctuations greater than +/- 5%. When this happens we unleash our quest in all different directions. Can it be the Learning Rate? Is the batch size too big or small? Is the model not generalizing enough?

However in several cases a bad dataset is the culprit. In this example, I am going to discuss one such case study that I did while debugging abnormal training and validation curve behavior. To set up the expectations in advance, the goal of this article is not to go over the coding aspect of how model was trained or how to write a customized CNN. There are several posts that talk about this, [Create CNN by Tensorflow.org](https://www.tensorflow.org/tutorials/images/cnn), [Building Custom CNN](https://odsc.medium.com/building-a-custom-convolutional-neural-network-in-keras-48171163aa7f). You will also need to create your own custom dataset generators. This blog will be helpful [Write your own custom dataset generator](https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3)

## So What was the Issue I faced?
Before, we go into the issue, let's visit the building blocks of our Deep Learning Framework, CNN architecture, datasets, model training, etc.

### The CNN architecture

In the current case study, I used a CNN to classify FontStyle, given an image with text. The design of the CNN was motivated by the [Deep Font Paper](https://arxiv.org/abs/1507.03196). Several modifications were made to the network architecture as per our problem requirements. My original project had 24 different font style classes but for this use case study, I will use only three to five different classes. A high level schema of the neural network architecture is shown below:
![ConVolutional Neural Net Architecture](images/CNN.png)

### The Dataset

Original dataset had 24 classes (font_styles). Each class had ~ 5000 and ~1000 samples in the training and validation set respectively. The data for each class is synthesized by writing fake text on a 100 by 100 white or noisy background patch using the font style of that class. The noisy background is generated by using cropped blank areas from scanned documents. Additional augmentation is applied to this background. We will discuss more about augmentation in the next section. Figures below show examples of training images generated using Open Source Google Fonts. From Left to Right: Arimo-Regular, Courier-Prime-Regular, Lato-Light and Tinos-Italic.

![Arimo Regular](images/Arimo_Regular.png)   ![CourierPrime Regular](images/courier_prime_regular.png)   ![Lato-Light](images/Lato-Light.png) ![Tinos-Italic](images/Tinos_Italic.png)

### Data Augmentation

Some standard data augmentation methods were applied on top of the synthetically generated data discussed in previous section. Following augmentation was used:
   1. GaussianNoise
   2. GaussianBlur
   3. Affine Rotation
   4. PerspectiveTransform
   5. Variation in Text Color
   6. Variation in Font Size

### Training Setup

Training was conducted for 200 epochs. A batch size of 64 was chosen. I chose a conservative learning rate of 1e-05. No early stopping was used initially. I used Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9 and decay rate of 1e-6. Since this a classification problem with labels in one hot encoded form, a Categorical Cross Entropy Loss was chosen.

## The Problem

When the training started, The loss decreased for both training and validation. And then suddenly when everything was going fine, at around 50th epoch (50-55% validation accuracy), validation loss started to increase while training loss continued to decrease.
Not a very comforting situation! So what might be going wrong here?

<div style="text-align:center">
   <img src="images/problem_loss_curve.png" alt= “curves” width="400" height="300" title="Diverging Loss Curves">
</div>

When we see a behavior like this, the first reaction is "Model is Overfitting" or model is not able to generalize well. This does make sense since model will learn whatever you give it so training loss will decrease but it will not perform well on Validation curve.

## So What Is The Solution?

Once we conclude 'Overfitting', a very common tendency is to perform data augmentation on training set to make it more representative of training data. We start looking into regularization techniques such as increasing dropout % or adding additional dropout layers. We think about tinkering with batch size or lowering the learning rates. We tend to think if our model is too complex and think about reducing number of weights in layers. While in several cases the aforementioned techniques can be useful but some of these can lead into a rabbit hole and need significant time and effort. I too tried some of the above methods but it was not useful. In such cases, the most logical first step is to look at the data.

Since this data is synthetically generated  for each class using our own code, the data cannot be mislabelled (right?). I will answer this in the last.

Looking at entire data can be soul sucking. I took the poorly trained model above and used it for inference on validation dataset. Any misclassiifed data was analyzed. If you want to get deep into analyzing data, you can set confidence thresholds for best predicted class and analyze the images below that threshold further.

We should first check if their are images which are garbage and are not representative of real world data. One such example is shared below. Excessive augmentation has resulted in font size of the text being too small and the image being blurry and filled with noise. If your dataset is huge, scanning for such examples manually may be like finding a needle in hay stack and may not be practical. Looking at the garbage data is just a suggestion if the size of dataset is reasonable for manual scanning. The question, whether an image is garbage or not is also subjective in nature. For the case shared below, one can ask themselves if they will encounter images like this in real world for their application. If answer is 'Yes', it is better to keep these images in the dataset else it is wise to tune down your data augmentation techniques to avoid generating such images.

<div style="text-align:center">
   <img src="images/garbage_example.png" alt= “” width="100" height="100" title="garbage_example">
</div>


Then what next?

While looking at the data I noticed that some of the data from different classes look very close. My classification task deals with classifying font styles and some of these font styles can look very similar to human but CNN can be trained to detect these. But let's not forget that on top of font styles being very close, we are adding augmentation which can be making things worse. I thought it is good idea to verify if this is the cause.  So our leading **_hypothesis_** so far is that the data from two or more classes is similar or exactly same for reasons unknown for now.  Let's worry about reasons later.

The next step should be, with this hypothesis, can you recreate the problem?

### Let's Recreate the Problem

For this I decided to experiment with small number of classes instead of 24 classes that I used in the original training task. I started with classes that are not close at all. I chose Calibri light and Google fonts Tino Regular and Gelasio . As you can see from the data samples below, these classes are far apart. I removed data augmentation steps 1-4 outlined above and kept 5 and 6.

![Calibri_Light](images/Calibri_Light.png)           ![Tinos-Regular](images/Tinos-Regular.png)             ![Gelasio](images/Gelasio.png)

As per our  **_hypothesis_**, I will expect our network to converge. Upon training with original set of hyperparamters and the results were as expected.
<div style="text-align:center">
   <img src="images/three_classes.jpg" alt= “” width="400" height="250" title="Distant Classes Accuracy Curve">                <img src="images/three_classes_loss.jpg" alt= “” width="400" height="250" title="Distant Classes Loss Curve">
</div>


Next, I will try to prove my initial hypothesis, ie, some classes are too close. To test this, I added other variations of Calibri as I expect these variations to be very close to each other. I added Calibri Bold Italic and Calibri Regular.

So now our set of classes are:
Calibri Light, Calibri Regular, Calibri Bold Italic, Tinos-Regular and Gelasio. For data augmentation, I kept data augmentation#5 and #6 and removed 1-4 (same as previous experiment) and I can recreate the issue.

<div style="text-align:center">
   <img src="images/recreated_issue.jpg" alt= “” width="400" height="250" title="recreated_issue">         <img src="images/recreated_issue_loss_curve.png" alt= “” width="400" height="250" title="recreated_issue">
</div>

So are we implying model is not capable enough to distinguish between Calibri Regular and Calibri Light (close classes). At this stage, you may be tempted to look into model complexity or may want to train model with different set of hyperparameters.

### But Wait!

Before we go into this rabbit hole, lets look into our data augmentation. In these experiments, we have been using augmentation techniques 'Variation in Text Color'(#5) and 'Variation in Font Size'(#6). For our text color augmentation techniques, I was simply varying intensity of pixels (0-255). It is possible, I may be creating augmented samples for Calibri Regular that may resemble like Calibri Light or creating augmented samples for Calibri Light that resemble more like Calibri Regular.

Let's test this hypothesis by removing color augmentation, while keeping the five classes we tested above and augmentation#6. Model training hyperparameters are also kept same as previous experiments. Below is the result for training for 100 epochs. We can see after removal of color augmentation, both Training and Validation Accuracy start to converge and are running close. The loss curve converges nicely as well.

![Alt text](images/problem_solved.png "Title")

## Closing Comments

A second look at our dataset, prevented us from spending a lot of time on other complicated investigation routes. I realized I was polluting my own dataset.  This case study, also tells us that we should pick our data augmentation methods wisely. The data augmentation methods should be specific to your use case. Going with standard image data augmentation methods, without thorough reasoning should be strictly avoided!
